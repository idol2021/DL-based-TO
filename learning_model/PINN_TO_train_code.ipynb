{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c732d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.modules.utils import _pair, _quadruple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device} is available')\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Stores data on CPU and lets DataLoader move to device.\n",
    "    FC: (N, H, W, C_in)\n",
    "    DTO: (N, H, W, 1)\n",
    "    com: (N,) scalar compliance targets per sample\n",
    "    \"\"\"\n",
    "    def __init__(self,FC,DTO,com):\n",
    "        self.x_data = torch.cuda.FloatTensor(FC)\n",
    "        self.x_data = self.x_data.permute(0,3,1,2) #\n",
    "\n",
    "        self.y_data = torch.cuda.FloatTensor(DTO)\n",
    "        self.y_data = self.y_data.permute(0,3,1,2)\n",
    "\n",
    "        self.z_data = torch.cuda.FloatTensor(com)\n",
    "        \n",
    "        self.len = self.y_data.shape[0]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index], self.z_data[index]\n",
    "\n",
    "# proposed U-Net based encoder decoder network for training\n",
    "class CNN_top(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_top, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(4,32,2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32,32,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64,64,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64,128,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128,128,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128,256,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(256,256,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384,128,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128,128,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            )\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(192,64,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64,64,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(96,32,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32,32,3,padding=\"same\"),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.layer8 =  nn.Sequential(\n",
    "             nn.Conv2d(32,1,3,padding=\"same\"),\n",
    "             nn.Sigmoid(),\n",
    "            )\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(384,384,1,1,padding=0),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(192,192,1,1,padding=0),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(96,96,1,1,padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.avg = nn.AvgPool2d(3,1,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        \n",
    "        x = self.layer4(x3)\n",
    "        x = torch.cat([x,x3],dim=1) # concatenate layer 256+128 =384\n",
    "        \n",
    "        # topology CNN filter 1\n",
    "        fil1 = x\n",
    "        x = self.conv1(fil1)\n",
    "        ori1 = x\n",
    "        x = self.avg (fil1)\n",
    "        x = self.conv1(x)\n",
    "        x = torch.add(x,ori1)\n",
    "        \n",
    "        x = self.layer5(x)\n",
    "        x = torch.cat([x,x2],dim=1) # concatenate layer 128+64=192\n",
    "        \n",
    "        # topology CNN filter 2 (TO CNN filter 1 and 2 are most effective)\n",
    "        fil2 = x\n",
    "        x = self.conv2(fil2)\n",
    "        ori2 = x\n",
    "        x = self.avg (fil2)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.add(x,ori2)\n",
    "\n",
    "        x = self.layer6(x)\n",
    "        x = torch.cat([x,x1],dim=1) # concatenate layer 64+32=96 \n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "        eps = 1e-6\n",
    "        return x * (1 - eps) + eps # modified output layer for preventing numerical instability\n",
    "\n",
    "# constraint loss with MAE loss\n",
    "def con_loss(pre,real):\n",
    "    return torch.mean(abs(torch.mean(pre)-torch.mean(real))) \n",
    "\n",
    "# compliance loss with FEM and one random culculation per batch\n",
    "def comploss(DTO,FC,comp,penal=3,Emax=1,Emin=1e-9,nu=0.3): \n",
    "    # final version of compliance loss \n",
    "    # torch.set_default_dtype(torch.float64) (use)\n",
    "    \n",
    "    # input data\n",
    "    n = min(DTO.shape[0],FC.shape[0],comp.shape[0]) \n",
    "    m = random.randint(0,n)-1   # one random comploss calculation per batch\n",
    "    \n",
    "    # TO structure\n",
    "    DTO_x = DTO[m,0,:,:].T\n",
    "    nelx, nely = DTO_x.shape\n",
    "    x = torch.flatten(DTO_x)\n",
    "    \n",
    "    # Force maaping\n",
    "    Fx_map = FC[m, 0, :, :].T \n",
    "    Fy_map = FC[m, 1, :, :].T\n",
    "\n",
    "    Fx_vec = Fx_map.flatten()\n",
    "    Fy_vec = Fy_map.flatten()\n",
    "    \n",
    "    fx = torch.where(Fx_vec!=0)[0]*2\n",
    "    fy = torch.where(Fy_vec!=0)[0]*2+1\n",
    "    F_idx = torch.hstack((fx,fy))\n",
    "    \n",
    "    Bx = Fx_vec[torch.where(Fx_vec!=0)]\n",
    "    By = Fy_vec[torch.where(Fy_vec!=0)]\n",
    "    B = torch.hstack((Bx,By))\n",
    "    \n",
    "    C = F_idx.numel()\n",
    "    \n",
    "    BCM = FC[m, 3, :, :].T\n",
    "    l=len(torch.where(BCM==1)[0])\n",
    "    BC_c=torch.where(BCM==1)\n",
    "    BC_a=torch.zeros(2*l,dtype=int)\n",
    "\n",
    "    for i in range(l):\n",
    "        BC_a[2*i]=2*(BC_c[0][i]+1)*(BC_c[1][i]+1)-2\n",
    "        BC_a[2*i+1]=2*(BC_c[0][i]+1)*(BC_c[1][i]+1)-1\n",
    "\n",
    "    # K matrix setting\n",
    "    ndof = 2*(nelx+1)*(nely+1)\n",
    "    edofMat = torch.zeros((nelx*nely,8),dtype=int)\n",
    "    nodenrs = torch.transpose(torch.reshape(torch.arange((1+nelx)*(1+nely)),(1+nelx,1+nely)),1,0)\n",
    "    edofVec = torch.reshape(torch.transpose((2*nodenrs[:-1,:-1]+1),1,0),(nely*nelx,1))\n",
    "    edofMat = torch.tile(edofVec,(1,8))+torch.tile(torch.tensor([1, 2, 2*nely+3, 2*nely+4, 2*nely+1 ,2*nely+2, -1, 0]),(nelx*nely,1))\n",
    "    iK = torch.flatten(torch.kron(edofMat,torch.ones((8,1)))).cuda()\n",
    "    jK = torch.flatten(torch.kron(edofMat,torch.ones((1,8)))).cuda()\n",
    "\n",
    "    A11 = torch.transpose(torch.reshape(torch.tensor([12,  3, -6, -3,  3, 12,  3,  0, -6,  3, 12, -3, -3,  0, -3, 12]),(4,4)),1,0)\n",
    "    A12 = torch.transpose(torch.reshape(torch.tensor([-6, -3,  0,  3, -3, -6, -3, -6,  0, -3, -6,  3,  3, -6,  3, -6]),(4,4)),1,0)\n",
    "    B11 = torch.transpose(torch.reshape(torch.tensor([-4,  3, -2,  9,  3, -4, -9,  4, -2, -9, -4, -3,  9,  4, -3, -4]),(4,4)),1,0)\n",
    "    B12 = torch.transpose(torch.reshape(torch.tensor([ 2, -3,  4, -9, -3,  2,  9, -2,  4,  9,  2,  3, -9, -2,  3,  2]),(4,4)),1,0)\n",
    "    KE = 1/(1-nu**2)/24*(torch.vstack((torch.hstack((A11,A12)),torch.hstack((A12.T, A11))))\n",
    "                        +nu*torch.vstack((torch.hstack((B11,B12)),torch.hstack((B12.T, B11))))).cuda()\n",
    "\n",
    "    dofs = torch.arange(2*(nelx+1)*(nely+1))\n",
    "    fixed = BC_a\n",
    "    combined = torch.cat((dofs, fixed))\n",
    "    uniques, counts = combined.unique(return_counts=True)\n",
    "    free = uniques[counts == 1]\n",
    "\n",
    "    f = torch.zeros((ndof,C)).cuda()\n",
    "    u = torch.zeros((ndof,C)).cuda()\n",
    "\n",
    "    for j in range(C):\n",
    "          f[F_idx[j],j] = B[j]\n",
    "    sK = (torch.unsqueeze(KE.flatten(),0).T*(Emin+(x)**penal*(Emax-Emin))).T.flatten()\n",
    "    K = torch.sparse_coo_tensor(torch.vstack((iK,jK)),sK,[ndof,ndof]).to_dense()\n",
    "    \n",
    "    # Remove constrained dofs from matrix\n",
    "    K = (K[free,:][:,free])\n",
    "    # Solve system \n",
    "    if C==1:\n",
    "        u[free,0]=torch.linalg.solve(K,f[free,0])\n",
    "    else:\n",
    "        u[free,:]=torch.linalg.solve(K,f[free,:])\n",
    "\n",
    "    # Objective and sensitivity\n",
    "    ce=torch.ones(nely*nelx).cuda()\n",
    "    obj=0\n",
    "    KE = KE.to(torch.float64)\n",
    "    \n",
    "    for i in range(C):\n",
    "        ui = u[:,i]\n",
    "        mod_ui = ui[edofMat].reshape(nelx*nely,8).to(torch.float64)\n",
    "        ce = (torch.matmul(mod_ui,KE) * mod_ui).sum(1)\n",
    "        obj = obj+( (Emin+x**penal*(Emax-Emin))*ce ).sum()\n",
    "\n",
    "    return torch.relu(obj/comp[m]-1)\n",
    "\n",
    "# loss polt\n",
    "def plot_loss(trn_loss_list,val_loss_list,title='Total Loss',save='Pytorch Total Loss & test.png'):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.plot(trn_loss_list)\n",
    "    plt.plot(val_loss_list)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training data', 'Validation data'], loc=0)\n",
    "    plt.rc('font', size=40)        \n",
    "    plt.rc('axes', labelsize=50)   \n",
    "    plt.rc('xtick', labelsize=40) \n",
    "    plt.rc('ytick', labelsize=40) \n",
    "    plt.rc('legend', fontsize=30)  \n",
    "    plt.rc('figure', titlesize=200)\n",
    "    plt.savefig(save)\n",
    "\n",
    "def topNN(DTO_data_path, FC_data_path, com_data_path, save_path, num_epochs = 200, data_num = 2000,\n",
    "          lamda1 = 5*10**(-15),lamda2 = 10**(-5),mini_batch_size=128):\n",
    "    \n",
    "    # data load\n",
    "    DTO = np.load(DTO_data_path)[0:data_num,:,:,:]\n",
    "    FC = np.load(FC_data_path)[0:data_num,:,:,:]\n",
    "    com = np.load(com_data_path)[0:data_num]\n",
    "\n",
    "    # data split \n",
    "    DTO_train, DTO_test, com_train, com_test, FC_train, FC_test = \\\n",
    "                train_test_split(DTO,com,FC, test_size=0.2, shuffle=True)\n",
    "    DTO_train, DTO_val, com_train, com_val, FC_train, FC_val = \\\n",
    "                train_test_split(DTO_train,com_train,FC_train, test_size=0.2, shuffle=True)\n",
    "\n",
    "    tr_dataset = CustomDataset(FC_train,DTO_train,com_train)\n",
    "    val_dataset = CustomDataset(FC_val,DTO_val,com_val)\n",
    "    test_dataset = CustomDataset(FC_test,DTO_test,com_test)\n",
    "    \n",
    "    tr_dataloader = DataLoader(tr_dataset, batch_size = mini_batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size = mini_batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size = 1, shuffle=True)\n",
    "\n",
    "    cnn = CNN_top().to(device) # model \n",
    "    \n",
    "    # define the main loss function\n",
    "    loss_fun = nn.BCELoss()\n",
    "    \n",
    "    # define the optimizer\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), \n",
    "                                 lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "    os.makedirs(save_path)\n",
    "    os.chdir(save_path)\n",
    "    \n",
    "    f = open('torch_train_results.txt', 'w')\n",
    "    print('\\nlambda1: ',lamda1, file=f)\n",
    "    print('\\nlambda2: ',lamda2, file=f)\n",
    "    print('\\nbatch_size: ', mini_batch_size, file=f)\n",
    "\n",
    "    trn_loss_list = []\n",
    "    train_loss_list = []\n",
    "    train_comploss_list = []\n",
    "    train_conloss_list = []\n",
    "    \n",
    "    val_loss_list = []\n",
    "    vali_loss_list = []\n",
    "    vali_comploss_list = []\n",
    "    vali_conloss_list = []\n",
    "    \n",
    "    trigger_times = 0\n",
    "    patience = 10\n",
    "    start_time = time.time()\n",
    "    print(\"Start\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        trn_loss = 0.0\n",
    "        cnn.train()\n",
    "\n",
    "        for i, samples in enumerate(tr_dataloader):\n",
    "            x_train, y_train, z_train = samples\n",
    "            y_train.requires_grad=True\n",
    "            optimizer.zero_grad()\n",
    "            output = cnn.forward(x_train)\n",
    "            \n",
    "            train_loss = loss_fun(output,y_train)\n",
    "            train_comploss = comploss(output,x_train.detach(),z_train.detach())\n",
    "            train_conloss = con_loss(output,y_train)\n",
    "            \n",
    "            loss = train_loss+lamda1*train_comploss+lamda2*train_conloss\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for j, val in enumerate(val_dataloader):\n",
    "                x_val, y_val, z_val = val\n",
    "                val_output = cnn(x_val)\n",
    "\n",
    "                vali_loss = loss_fun(val_output,y_val)\n",
    "                vali_comploss = comploss(val_output,x_val.detach(),z_val.detach())\n",
    "                vali_conloss = con_loss(val_output,y_val)\n",
    "\n",
    "                v_loss = vali_loss+lamda1*vali_comploss+lamda2*vali_conloss\n",
    "                val_loss += v_loss\n",
    "\n",
    "            print((\"epoch: {}/{} \\ntrain: | tot loss: {:.4f} | BCE loss: {:.4f} | comp loss: {:.4f} |\"\n",
    "                   +\" cons loss: {:.4f} | \\nvalid: | tot loss: {:.4f} | BCE loss: {:.4f} |\"+\" comp loss: {:.4f} | cons loss: {:.4f} |\")\n",
    "                  .format(epoch+1, num_epochs,\n",
    "              loss.item(),\n",
    "              train_loss.item(),\n",
    "              train_comploss.item(),   # <- () 빠졌던 부분\n",
    "              train_conloss.item(),\n",
    "              v_loss.item(),\n",
    "              vali_loss.item(),\n",
    "              vali_comploss.item(),    # <- () 빠졌던 부분\n",
    "              vali_conloss.item()))\n",
    "            \n",
    "            trn_loss_list.append(loss.item())\n",
    "            train_loss_list.append(train_loss.item())\n",
    "            train_comploss_list.append(train_comploss.item())\n",
    "            train_conloss_list.append(train_conloss.item())\n",
    "        \n",
    "            val_loss_list.append(v_loss.item())\n",
    "            vali_loss_list.append(vali_loss.item())\n",
    "            vali_comploss_list.append(vali_comploss.item())\n",
    "            vali_conloss_list.append(vali_conloss.item())\n",
    "\n",
    "        # Early Stopping criterion patence\n",
    "        if epoch>1:\n",
    "            if val_loss_list[epoch]>val_loss_list[epoch-1]:\n",
    "                trigger_times +=1\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping! {}'.format(epoch+1))\n",
    "                    break\n",
    "            else:\n",
    "                trigger_times = 0\n",
    "        torch.save(cnn, 'model'+str(epoch)+'.pt') \n",
    "    print(\"End\")\n",
    "    print(\"Time: {:.4f}sec\".format((time.time() - start_time)))\n",
    "\n",
    "    print('\\nTotal iteration: {}/{}'.format(epoch+1, num_epochs), file=f)\n",
    "    print('\\nTraining time: {:.4f}sec'.format((time.time() - start_time)), file=f)\n",
    "    \n",
    "    plot_loss(trn_loss_list,val_loss_list,'Total Loss','Pytorch Total Loss & test1.png')\n",
    "    plot_loss(train_loss_list,vali_loss_list,'BCE Loss','Pytorch BCE Loss & test.png')\n",
    "    plot_loss(train_comploss_list,vali_comploss_list,'comp Loss','Pytorch comp Loss & test.png')\n",
    "    plot_loss(train_conloss_list,vali_conloss_list,'con Loss','Pytorch con Loss & test.png')\n",
    "    plot_loss(trn_loss_list,val_loss_list,'Total Loss','Pytorch Total Loss & test.png')\n",
    "\n",
    "    test_loss_fun = nn.L1Loss() \n",
    "\n",
    "    cnn.eval()\n",
    "    start_time = time.time()\n",
    "    print(\"Start\")\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_comploss = 0.0\n",
    "    test_conloss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, samples in enumerate(test_dataloader):\n",
    "            x_test, y_test, z_test = samples\n",
    "            test_output = cnn(x_test)\n",
    "            test_loss += test_loss_fun(test_output,y_test)\n",
    "            test_comploss += comploss(test_output, x_test.detach(), z_test.detach())\n",
    "            test_conloss += con_loss(torch.mean(test_output),torch.mean(y_test))\n",
    "\n",
    "            if i == 0: \n",
    "                test_out = test_output\n",
    "                x_test_out = x_test\n",
    "                y_test_out = y_test\n",
    "                z_test_out = z_test\n",
    "            else:\n",
    "                test_out = torch.cat([test_out,test_output], dim=0)\n",
    "                x_test_out = torch.cat([x_test_out,x_test], dim=0)\n",
    "                y_test_out = torch.cat([y_test_out,y_test], dim=0)\n",
    "                z_test_out = torch.cat([z_test_out,z_test], dim=0)\n",
    "\n",
    "    print(\"End\")\n",
    "    print(\"Time: {:.4f}sec\".format((time.time() - start_time)))\n",
    "    print(\"test loss: {:.4f} | comp loss: {:.4f} | cons loss: {:.4f}\"\n",
    "          .format(test_loss/(i+1),test_comploss/(i+1),test_conloss/(i+1)))\n",
    "    print(\"\\noriginal test loss: {:.4f} | comp loss: {:.4f} | cons loss: {:.4f}\"\n",
    "          .format(test_loss/(i+1),test_comploss/(i+1),test_conloss/(i+1)), file=f)\n",
    "    \n",
    "    accuracy_test = abs(test_out - y_test_out)\n",
    "    accuracy_test = accuracy_test.cpu().numpy()\n",
    "    acc = len(accuracy_test[np.where(accuracy_test<0.5)])/test_out.cpu().numpy().size*100\n",
    "    print(acc, file=f)\n",
    "\n",
    "    save_test = test_out\n",
    "    save_test = save_test.permute(0,2,3,1)\n",
    "    save_test = save_test.detach().cpu().numpy()\n",
    "\n",
    "    save_x_test = x_test_out\n",
    "    save_x_test = save_x_test.permute(0,2,3,1)\n",
    "    save_x_test = save_x_test.detach().cpu().numpy()\n",
    "\n",
    "    save_y_test = y_test_out\n",
    "    save_y_test = save_y_test.permute(0,2,3,1)\n",
    "    save_y_test = save_y_test.detach().cpu().numpy()\n",
    "\n",
    "    save_z_test = z_test_out\n",
    "    save_z_test = save_z_test.detach().cpu().numpy()\n",
    "\n",
    "    np.save('save_new_test.npy',save_test)\n",
    "    np.save('save_x_new_test.npy',save_x_test)\n",
    "    np.save('save_y_new_test.npy',save_y_test)\n",
    "    np.save('save_z_new_test.npy',save_z_test)\n",
    "\n",
    "    torch.save(cnn, 'model.pt')  # save the model \n",
    "    torch.save(cnn.state_dict(), 'model_state_dict.pt')  # save the state_dict of the model\n",
    "    torch.save({\n",
    "        'model': cnn.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }, 'all.tar')  \n",
    "    f.close()\n",
    "    return test_loss/(i+1), test_comploss/(i+1), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7157a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = []\n",
    "test_comploss = []\n",
    "test_acc = []\n",
    "\n",
    "epoch = 200\n",
    "data_num = 2000\n",
    "batch_size = 32\n",
    "\n",
    "DTO_data_path = 'newDTO_3232_cantilver_beam_20k.npy'\n",
    "com_data_path = 'newcomp_3232_cantilver_beam_20k.npy'\n",
    "FC_data_path = 'newFCv_3232_cantilver_beam_20k.npy'\n",
    "\n",
    "save_path = 'C:\\\\Users\\\\user\\\\'\n",
    "name = \"2000data_32x32_new_additional_test\"\n",
    "\n",
    "a = topNN(DTO_data_path, FC_data_path, com_data_path,\n",
    "          save_path+name,\n",
    "          epoch,data_num, 10**(-5), 10**(-4), batch_size)\n",
    "\n",
    "test_loss.append(a[0])\n",
    "test_comploss.append(a[1])\n",
    "test_acc.append(a[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b2c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
